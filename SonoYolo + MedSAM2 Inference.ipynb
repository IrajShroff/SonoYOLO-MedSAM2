{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfba201-154f-4b47-8ce3-088d6df3a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fed2cd-e191-4e5e-a24a-172c3cf78826",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c7599-8a4d-4c68-a024-2d8b67ae9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "video_folder = \"/EchoNet-Dynamic/Videos\"\n",
    "csv_path = \"/EchoNet-Dynamic/VolumeTracings.csv\"\n",
    "\n",
    "# Create the folder that will hold outputs (not the CSV file!)\n",
    "output_root = \"/EchoNet-Dynamic/dataset_frames\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "print(\"Video folder exists:\", os.path.exists(video_folder))\n",
    "print(\"CSV file exists:\", os.path.isfile(csv_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f011728-9767-42d7-877c-795857b3a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./checkpoints/\n",
    "!curl https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt --output checkpoints.pt #change to large when need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5735f0-af7f-4d98-bcb1-95d8c3456588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25c16f-88e4-4725-aa21-2d83421f7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb91fb6-1646-4371-aa53-11d90f1238ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1e24e-8e86-40b5-96c2-5b8fa22bca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Always stay inside your home folder\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"videos\", exist_ok=True)\n",
    "\n",
    "# Download model checkpoint\n",
    "model_url = \"https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\"\n",
    "model_path = \"checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "if not os.path.exists(model_path):\n",
    "    urllib.request.urlretrieve(model_url, model_path)\n",
    "    print(\"Downloaded model checkpoint\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37090d7-f94b-410a-ae4e-7b8d0f8b7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"/MedSAM2/checkpoints/MedSAM2_latest.pt\"\n",
    "model_cfg = \"configs/sam2.1_hiera_t512.yaml\"\n",
    "\n",
    "#predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "predictor = build_sam2_video_predictor(\n",
    "    config_file=model_cfg,\n",
    "    ckpt_path= sam2_checkpoint,\n",
    "    apply_postprocessing=True,\n",
    "    # hydra_overrides_extra=hydra_overrides_extra,\n",
    "    vos_optimized=  True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32949d34-5f2b-4566-bae1-51726058af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_folder = \"/EchoNet-Dynamic/Videos\"      # input: where .avi videos are\n",
    "output_root = \"EchoNet-Dynamic/dataset_frames\"       # output: where frames will go\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# list of .avi files\n",
    "avi_files = [f for f in os.listdir(video_folder) if f.endswith(\".avi\")]\n",
    "avi_files.sort()  \n",
    "\n",
    "# Process only the first video\n",
    "video_file = avi_files[0]\n",
    "video_path = os.path.join(video_folder, video_file)\n",
    "video_name = os.path.splitext(video_file)[0]\n",
    "\n",
    "\n",
    "video_dir = os.path.join(output_root, video_name)  # == where .jpg frames are saved\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# Extract frames\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_idx = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_path = os.path.join(video_dir, f\"{frame_idx}.jpg\")  # use `video_dir` here\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "print(f\"Extracted {frame_idx} frames from '{video_file}' into '{video_dir}'\")\n",
    "\n",
    "# List and sort frame files from the video_dir\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# Show the first frame (just like the original code)\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0360ec5-5c7e-477c-929d-aa4141cc13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)\n",
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8ff5-6fae-4e6b-b463-51c959a089f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EchoNet-Dynamic Dataset.\"\"\" #Code from this cell obtained from https://github.com/echonet/dynamic/blob/master/echonet/datasets/echo.py\n",
    "import os\n",
    "import collections\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import torchvision\n",
    "import echonet\n",
    "\n",
    "\n",
    "class Echo(torchvision.datasets.VisionDataset):\n",
    "    \"\"\"EchoNet-Dynamic Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset (defaults to `echonet.config.DATA_DIR`)\n",
    "        split (string): One of {``train'', ``val'', ``test'', ``all'', or ``external_test''}\n",
    "        target_type (string or list, optional): Type of target to use,\n",
    "            ``Filename'', ``EF'', ``EDV'', ``ESV'', ``LargeIndex'',\n",
    "            ``SmallIndex'', ``LargeFrame'', ``SmallFrame'', ``LargeTrace'',\n",
    "            or ``SmallTrace''\n",
    "            Can also be a list to output a tuple with all specified target types.\n",
    "            The targets represent:\n",
    "                ``Filename'' (string): filename of video\n",
    "                ``EF'' (float): ejection fraction\n",
    "                ``EDV'' (float): end-diastolic volume\n",
    "                ``ESV'' (float): end-systolic volume\n",
    "                ``LargeIndex'' (int): index of large (diastolic) frame in video\n",
    "                ``SmallIndex'' (int): index of small (systolic) frame in video\n",
    "                ``LargeFrame'' (np.array shape=(3, height, width)): normalized large (diastolic) frame\n",
    "                ``SmallFrame'' (np.array shape=(3, height, width)): normalized small (systolic) frame\n",
    "                ``LargeTrace'' (np.array shape=(height, width)): left ventricle large (diastolic) segmentation\n",
    "                    value of 0 indicates pixel is outside left ventricle\n",
    "                             1 indicates pixel is inside left ventricle\n",
    "                ``SmallTrace'' (np.array shape=(height, width)): left ventricle small (systolic) segmentation\n",
    "                    value of 0 indicates pixel is outside left ventricle\n",
    "                             1 indicates pixel is inside left ventricle\n",
    "            Defaults to ``EF''.\n",
    "        mean (int, float, or np.array shape=(3,), optional): means for all (if scalar) or each (if np.array) channel.\n",
    "            Used for normalizing the video. Defaults to 0 (video is not shifted).\n",
    "        std (int, float, or np.array shape=(3,), optional): standard deviation for all (if scalar) or each (if np.array) channel.\n",
    "            Used for normalizing the video. Defaults to 0 (video is not scaled).\n",
    "        length (int or None, optional): Number of frames to clip from video. If ``None'', longest possible clip is returned.\n",
    "            Defaults to 16.\n",
    "        period (int, optional): Sampling period for taking a clip from the video (i.e. every ``period''-th frame is taken)\n",
    "            Defaults to 2.\n",
    "        max_length (int or None, optional): Maximum number of frames to clip from video (main use is for shortening excessively\n",
    "            long videos when ``length'' is set to None). If ``None'', shortening is not applied to any video.\n",
    "            Defaults to 250.\n",
    "        clips (int, optional): Number of clips to sample. Main use is for test-time augmentation with random clips.\n",
    "            Defaults to 1.\n",
    "        pad (int or None, optional): Number of pixels to pad all frames on each side (used as augmentation).\n",
    "            and a window of the original size is taken. If ``None'', no padding occurs.\n",
    "            Defaults to ``None''.\n",
    "        noise (float or None, optional): Fraction of pixels to black out as simulated noise. If ``None'', no simulated noise is added.\n",
    "            Defaults to ``None''.\n",
    "        target_transform (callable, optional): A function/transform that takes in the target and transforms it.\n",
    "        external_test_location (string): Path to videos to use for external testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root=None,\n",
    "                 split=\"train\", target_type=\"EF\",\n",
    "                 mean=0., std=1.,\n",
    "                 length=16, period=2,\n",
    "                 max_length=250,\n",
    "                 clips=1,\n",
    "                 pad=None,\n",
    "                 noise=None,\n",
    "                 target_transform=None,\n",
    "                 external_test_location=None):\n",
    "        if root is None:\n",
    "            root = \"/shared_data/p_vidalr/iraj/EchoNet-Dynamic\"\n",
    "\n",
    "        super().__init__(root, target_transform=target_transform)\n",
    "\n",
    "        self.split = split.upper()\n",
    "        if not isinstance(target_type, list):\n",
    "            target_type = [target_type]\n",
    "        self.target_type = target_type\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.length = length\n",
    "        self.max_length = max_length\n",
    "        self.period = period\n",
    "        self.clips = clips\n",
    "        self.pad = pad\n",
    "        self.noise = noise\n",
    "        self.target_transform = target_transform\n",
    "        self.external_test_location = external_test_location\n",
    "\n",
    "        self.fnames, self.outcome = [], []\n",
    "\n",
    "        if self.split == \"EXTERNAL_TEST\":\n",
    "            self.fnames = sorted(os.listdir(self.external_test_location))\n",
    "        else:\n",
    "            # Load video-level labels\n",
    "            with open(os.path.join(self.root, \"FileList.csv\")) as f:\n",
    "                data = pandas.read_csv(f)\n",
    "            data[\"Split\"].map(lambda x: x.upper())\n",
    "\n",
    "            if self.split != \"ALL\":\n",
    "                data = data[data[\"Split\"] == self.split]\n",
    "\n",
    "            self.header = data.columns.tolist()\n",
    "            self.fnames = data[\"FileName\"].tolist()\n",
    "            self.fnames = [fn + \".avi\" for fn in self.fnames if os.path.splitext(fn)[1] == \"\"]  # Assume avi if no suffix\n",
    "            self.outcome = data.values.tolist()\n",
    "\n",
    "            # Check that files are present\n",
    "            missing = set(self.fnames) - set(os.listdir(os.path.join(self.root, \"Videos\")))\n",
    "            if len(missing) != 0:\n",
    "                print(\"{} videos could not be found in {}:\".format(len(missing), os.path.join(self.root, \"Videos\")))\n",
    "                for f in sorted(missing):\n",
    "                    print(\"\\t\", f)\n",
    "                raise FileNotFoundError(os.path.join(self.root, \"Videos\", sorted(missing)[0]))\n",
    "\n",
    "            # Load tracings\n",
    "            self.frames = collections.defaultdict(list)\n",
    "            self.trace = collections.defaultdict(_defaultdict_of_lists)\n",
    "\n",
    "            with open(os.path.join(self.root, \"VolumeTracings.csv\")) as f:\n",
    "                header = f.readline().strip().split(\",\")\n",
    "                assert header == [\"FileName\", \"X1\", \"Y1\", \"X2\", \"Y2\", \"Frame\"]\n",
    "\n",
    "                for line in f:\n",
    "                    filename, x1, y1, x2, y2, frame = line.strip().split(',')\n",
    "                    x1 = float(x1)\n",
    "                    y1 = float(y1)\n",
    "                    x2 = float(x2)\n",
    "                    y2 = float(y2)\n",
    "                    frame = int(frame)\n",
    "                    if frame not in self.trace[filename]:\n",
    "                        self.frames[filename].append(frame)\n",
    "                    self.trace[filename][frame].append((x1, y1, x2, y2))\n",
    "            for filename in self.frames:\n",
    "                for frame in self.frames[filename]:\n",
    "                    self.trace[filename][frame] = np.array(self.trace[filename][frame])\n",
    "\n",
    "            # A small number of videos are missing traces; remove these videos\n",
    "            keep = [len(self.frames[f]) >= 2 for f in self.fnames]\n",
    "            self.fnames = [f for (f, k) in zip(self.fnames, keep) if k]\n",
    "            self.outcome = [f for (f, k) in zip(self.outcome, keep) if k]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Find filename of video\n",
    "        if self.split == \"EXTERNAL_TEST\":\n",
    "            video = os.path.join(self.external_test_location, self.fnames[index])\n",
    "        elif self.split == \"CLINICAL_TEST\":\n",
    "            video = os.path.join(self.root, \"ProcessedStrainStudyA4c\", self.fnames[index])\n",
    "        else:\n",
    "            video = os.path.join(self.root, \"Videos\", self.fnames[index])\n",
    "\n",
    "        # Load video into np.array\n",
    "        video = echonet.utils.loadvideo(video).astype(np.float32)\n",
    "\n",
    "        # Add simulated noise (black out random pixels)\n",
    "        # 0 represents black at this point (video has not been normalized yet)\n",
    "        if self.noise is not None:\n",
    "            n = video.shape[1] * video.shape[2] * video.shape[3]\n",
    "            ind = np.random.choice(n, round(self.noise * n), replace=False)\n",
    "            f = ind % video.shape[1]\n",
    "            ind //= video.shape[1]\n",
    "            i = ind % video.shape[2]\n",
    "            ind //= video.shape[2]\n",
    "            j = ind\n",
    "            video[:, f, i, j] = 0\n",
    "\n",
    "        # Apply normalization\n",
    "        if isinstance(self.mean, (float, int)):\n",
    "            video -= self.mean\n",
    "        else:\n",
    "            video -= self.mean.reshape(3, 1, 1, 1)\n",
    "\n",
    "        if isinstance(self.std, (float, int)):\n",
    "            video /= self.std\n",
    "        else:\n",
    "            video /= self.std.reshape(3, 1, 1, 1)\n",
    "\n",
    "        # Set number of frames\n",
    "        c, f, h, w = video.shape\n",
    "        if self.length is None:\n",
    "            # Take as many frames as possible\n",
    "            length = f // self.period\n",
    "        else:\n",
    "            # Take specified number of frames\n",
    "            length = self.length\n",
    "\n",
    "        if self.max_length is not None:\n",
    "            # Shorten videos to max_length\n",
    "            length = min(length, self.max_length)\n",
    "\n",
    "        if f < length * self.period:\n",
    "            # Pad video with frames filled with zeros if too short\n",
    "            # 0 represents the mean color (dark grey), since this is after normalization\n",
    "            video = np.concatenate((video, np.zeros((c, length * self.period - f, h, w), video.dtype)), axis=1)\n",
    "            c, f, h, w = video.shape  # pylint: disable=E0633\n",
    "\n",
    "        if self.clips == \"all\":\n",
    "            # Take all possible clips of desired length\n",
    "            start = np.arange(f - (length - 1) * self.period)\n",
    "        else:\n",
    "            # Take random clips from video\n",
    "            start = np.random.choice(f - (length - 1) * self.period, self.clips)\n",
    "\n",
    "        # Gather targets\n",
    "        target = []\n",
    "        for t in self.target_type:\n",
    "            key = self.fnames[index]\n",
    "            if t == \"Filename\":\n",
    "                target.append(self.fnames[index])\n",
    "            elif t == \"LargeIndex\":\n",
    "                # Traces are sorted by cross-sectional area\n",
    "                # Largest (diastolic) frame is last\n",
    "                target.append(np.int(self.frames[key][-1]))\n",
    "            elif t == \"SmallIndex\":\n",
    "                # Largest (diastolic) frame is first\n",
    "                target.append(np.int(self.frames[key][0]))\n",
    "            elif t == \"LargeFrame\":\n",
    "                target.append(video[:, self.frames[key][-1], :, :])\n",
    "            elif t == \"SmallFrame\":\n",
    "                target.append(video[:, self.frames[key][0], :, :])\n",
    "            elif t in [\"LargeTrace\", \"SmallTrace\"]:\n",
    "                if t == \"LargeTrace\":\n",
    "                    t = self.trace[key][self.frames[key][-1]]\n",
    "                else:\n",
    "                    t = self.trace[key][self.frames[key][0]]\n",
    "                x1, y1, x2, y2 = t[:, 0], t[:, 1], t[:, 2], t[:, 3]\n",
    "                x = np.concatenate((x1[1:], np.flip(x2[1:])))\n",
    "                y = np.concatenate((y1[1:], np.flip(y2[1:])))\n",
    "\n",
    "                r, c = skimage.draw.polygon(np.rint(y).astype(int), np.rint(x).astype(int), (video.shape[2], video.shape[3]))\n",
    "\n",
    "\n",
    "                mask = np.zeros((video.shape[2], video.shape[3]), np.float32)\n",
    "                mask[r, c] = 1\n",
    "                target.append(mask)\n",
    "            else:\n",
    "                if self.split == \"CLINICAL_TEST\" or self.split == \"EXTERNAL_TEST\":\n",
    "                    target.append(np.float32(0))\n",
    "                else:\n",
    "                    target.append(np.float32(self.outcome[index][self.header.index(t)]))\n",
    "\n",
    "        if target != []:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "        # Select clips from video\n",
    "        video = tuple(video[:, s + self.period * np.arange(length), :, :] for s in start)\n",
    "        if self.clips == 1:\n",
    "            video = video[0]\n",
    "        else:\n",
    "            video = np.stack(video)\n",
    "\n",
    "        if self.pad is not None:\n",
    "            # Add padding of zeros (mean color of videos)\n",
    "            # Crop of original size is taken out\n",
    "            # (Used as augmentation)\n",
    "            c, l, h, w = video.shape\n",
    "            temp = np.zeros((c, l, h + 2 * self.pad, w + 2 * self.pad), dtype=video.dtype)\n",
    "            temp[:, :, self.pad:-self.pad, self.pad:-self.pad] = video  # pylint: disable=E1130\n",
    "            i, j = np.random.randint(0, 2 * self.pad, 2)\n",
    "            video = temp[:, :, i:(i + h), j:(j + w)]\n",
    "\n",
    "        return video, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Additional information to add at end of __repr__.\"\"\"\n",
    "        lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)\n",
    "\n",
    "\n",
    "def _defaultdict_of_lists():\n",
    "    \"\"\"Returns a defaultdict of lists.\n",
    "\n",
    "    This is used to avoid issues with Windows (if this function is anonymous,\n",
    "    the Echo dataset cannot be used in a dataloader).\n",
    "    \"\"\"\n",
    "\n",
    "    return collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597425c7-7e4b-4595-891b-ac8497b898e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from echonet.datasets import Echo\n",
    "\n",
    "#manual_mask means ground truth mask\n",
    "def save_manual_masks(filename, dataset_root=\"/EchoNet-Dynamic\",\n",
    "                      output_dir=\"/EchoNet-Dynamic/manual_masks\",\n",
    "                      show_plot=False):\n",
    "    \"\"\"\n",
    "    Save ED and ES binary masks for a given EchoNet-Dynamic video.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Name of the video file (e.g. \"0XF0B7D7CD42C001E.avi\")\n",
    "    dataset_root : str\n",
    "        Path to the EchoNet-Dynamic dataset root\n",
    "    output_dir : str\n",
    "        Directory where masks will be saved (one subfolder per video)\n",
    "    show_plot : bool\n",
    "        If True, displays ED/ES overlays\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Load dataset ---\n",
    "    dataset = Echo(\n",
    "        root=dataset_root,\n",
    "        split=\"test\",\n",
    "        target_type=[\"LargeFrame\", \"LargeTrace\", \"SmallFrame\", \"SmallTrace\"],\n",
    "        length=None,\n",
    "        clips=1\n",
    "    )\n",
    "   \n",
    "    index = dataset.fnames.index(filename)\n",
    "    video, (large_frame, large_mask, small_frame, small_mask) = dataset[index]\n",
    "\n",
    "    # --- Get ED/ES frame indices ---\n",
    "    frame_list = dataset.frames[filename]\n",
    "    es_index = int(frame_list[0])   # ES\n",
    "    ed_index = int(frame_list[-1])  # ED\n",
    "\n",
    "    ed_mask = (large_mask > 0).astype(np.uint8)\n",
    "    es_mask = (small_mask > 0).astype(np.uint8)\n",
    "\n",
    "    video_out_dir = os.path.join(output_dir, filename)\n",
    "    os.makedirs(video_out_dir, exist_ok=True)\n",
    "\n",
    "    ed_path = os.path.join(video_out_dir, f\"frame_{ed_index:03d}_manual_mask.png\")\n",
    "    es_path = os.path.join(video_out_dir, f\"frame_{es_index:03d}_manual_mask.png\")\n",
    "\n",
    "    Image.fromarray(ed_mask * 255).save(ed_path)\n",
    "    Image.fromarray(es_mask * 255).save(es_path)\n",
    "\n",
    "    print(f\"[Saved] ED → {ed_path}\\n        ES → {es_path}\")\n",
    "\n",
    "    # --- Plot if requested ---\n",
    "    if show_plot:\n",
    "        def prep_frame(frame_chw):\n",
    "            f = np.transpose(frame_chw, (1, 2, 0))\n",
    "            f = (f - f.min()) / (f.max() - f.min() + 1e-8)\n",
    "            return f\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(prep_frame(large_frame), cmap=\"gray\")\n",
    "        plt.imshow(ed_mask, cmap=\"Reds\", alpha=0.4)\n",
    "        plt.title(f\"ED overlay (frame {ed_index})\"); plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(prep_frame(small_frame), cmap=\"gray\")\n",
    "        plt.imshow(es_mask, cmap=\"Blues\", alpha=0.4)\n",
    "        plt.title(f\"ES overlay (frame {es_index})\"); plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3015628-50cc-4db0-800d-bae31880770a",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24275424-b4d7-48bc-8e7d-61dbe0a759b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dice_score(mask1, mask2):\n",
    "    mask1 = (mask1 > 0).astype(np.bool_)\n",
    "    mask2 = (mask2 > 0).astype(np.bool_)\n",
    "    intersection = np.sum(mask1 & mask2)\n",
    "    total = np.sum(mask1) + np.sum(mask2)\n",
    "    return 1.0 if total == 0 else 2.0 * intersection / total\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def evaluate_medsam2_on_video(video_filename, predictor,\n",
    "                               tracings_csv=\"/EchoNet-Dynamic/VolumeTracings.csv\",\n",
    "                               video_dir=\"/EchoNet-Dynamic/Videos\",\n",
    "                               frames_root=\"/EchoNet-Dynamic/dataset_frames\", #You will need to do this on your own, extract the frames for every single video and save them in their respective folder\n",
    "                               masks_root=\"/EchoNet-Dynamic/where you want to save the masks from this experiment\", #you set the name of this yourself\n",
    "                               gt_mask_root=\"/EchoNet-Dynamic/manual_masks\", #this is where ground truth is saved\n",
    "                               yolo_labels_dir=\"/runs/detect/predict/labels\",  #running Sono_YOLO.ipynb code will give you this file\n",
    "                               image_size=128):\n",
    "    \n",
    "    original_size = 112\n",
    "    padded_size = image_size\n",
    "    pad = (padded_size - original_size) // 2\n",
    "\n",
    "    print(f\"\\n========== Processing Video: {video_filename} ==========\")\n",
    "\n",
    "    video_id = os.path.splitext(video_filename)[0]\n",
    "    base_name = video_id\n",
    "    j_name = video_id + '.avi'\n",
    "    video_path = os.path.join(video_dir, video_filename)\n",
    "    frames_dir = os.path.join(frames_root, j_name)\n",
    "    masks_dir = os.path.join(masks_root, base_name)\n",
    "\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "    print(\"[INFO] Loading VolumeTracings.csv...\")\n",
    "    df = pd.read_csv(tracings_csv)\n",
    "    df_video = df[df['FileName'] == video_filename]\n",
    "    unique_frames = sorted(df_video['Frame'].unique())\n",
    "\n",
    "    if len(unique_frames) < 2:\n",
    "        print(f\"[WARNING] Not enough annotated frames in {base_name}\")\n",
    "        return\n",
    "\n",
    "    first_idx, second_idx = unique_frames[0], unique_frames[1]\n",
    "    print(f\"[INFO] First annotated frame: {first_idx}, Second annotated frame: {second_idx}\")\n",
    "\n",
    "    # =========== Unpad YOLO Prediction to Original Frame Size ============\n",
    "    label_filename = f\"{video_id}_frame{first_idx}.txt\"\n",
    "    label_path = os.path.join(yolo_labels_dir, label_filename)\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"[ERROR] YOLO label file not found: {label_path}\")\n",
    "        return\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        line = f.readline().strip()\n",
    "        parts = line.split()\n",
    "        if len(parts) != 5:\n",
    "            print(f\"[ERROR] Invalid YOLO label format in {label_path}\")\n",
    "            return\n",
    "        _, x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts)\n",
    "\n",
    "    # Convert to 128x128 pixel coordinates\n",
    "    x_center_128 = x_center_norm * padded_size\n",
    "    y_center_128 = y_center_norm * padded_size\n",
    "    width_128 = width_norm * padded_size\n",
    "    height_128 = height_norm * padded_size\n",
    "\n",
    "    # Shift center back by -pad to align with 112x112 original frame\n",
    "    x_center_112 = x_center_128 - pad\n",
    "    y_center_112 = y_center_128 - pad\n",
    "\n",
    "    # Final box in original 112x112 space\n",
    "    xmin = x_center_112 - width_128 / 2\n",
    "    ymin = y_center_112 - height_128 / 2\n",
    "    xmax = x_center_112 + width_128 / 2\n",
    "    ymax = y_center_112 + height_128 / 2\n",
    "    box = np.array([xmin + 3, ymin + 3, xmax - 3, ymax - 3], dtype=np.float32)\n",
    "\n",
    "    print(f\"[INFO] Unpadded YOLO bbox for MedSAM2: {box}\")\n",
    "    # =====================================================================\n",
    "\n",
    "    # Step 1: Extract frames\n",
    "    if len(os.listdir(frames_dir)) == 0:\n",
    "        print(f\"[INFO] Extracting frames from {video_path}...\")\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_path = os.path.join(frames_dir, f\"{idx}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            idx += 1\n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {idx} frames.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Frames already present in {frames_dir}\")\n",
    "\n",
    "    # Step 2: Initialize and run MedSAM2\n",
    "    print(f\"[INFO] Initializing MedSAM2 predictor...\")\n",
    "    inference_state = predictor.init_state(video_path=frames_dir)\n",
    "    predictor.reset_state(inference_state)\n",
    "\n",
    "    print(f\"[INFO] Running MedSAM2 at frame {first_idx}...\")\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=first_idx,\n",
    "        obj_id=1,\n",
    "        box=box,\n",
    "    )\n",
    "\n",
    "    # Step 3: Propagate masks\n",
    "    print(\"[INFO] Propagating masks through the video...\")\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            mask = (out_mask_logits[i] > 0).cpu().numpy().squeeze().astype(np.uint8) * 255\n",
    "            mask_path = os.path.join(masks_dir, f\"frame_{out_frame_idx}_sam_mask.png\")\n",
    "            Image.fromarray(mask).save(mask_path)\n",
    "            if out_frame_idx % 30 == 0:\n",
    "                print(f\"[DEBUG] Saved MedSAM2 mask at frame {out_frame_idx}\")\n",
    "\n",
    "    # Step 4: Generate ground truth mask(s) for this video\n",
    "    print(f\"[INFO] Generating ground truth mask(s) for {video_filename}...\")\n",
    "    save_manual_masks(video_filename)\n",
    "        \n",
    "    \n",
    "    # load ground truth mask for the second annotated frame\n",
    "    gt_mask_path = os.path.join(gt_mask_root, video_filename, f\"frame_{second_idx:03d}_manual_mask.png\")\n",
    "    if not os.path.exists(gt_mask_path):\n",
    "        print(f\"[ERROR] Ground truth mask not found at {gt_mask_path}\")\n",
    "        return\n",
    "    gt_mask = np.array(Image.open(gt_mask_path).convert('L'))\n",
    "    print(f\"[INFO] Loaded ground truth mask from {gt_mask_path}\")\n",
    "\n",
    "    # Step 5: Load MedSAM2 mask\n",
    "    pred_mask_path = os.path.join(masks_dir, f\"frame_{second_idx}_sam_mask.png\")\n",
    "    if not os.path.exists(pred_mask_path):\n",
    "        print(f\"[ERROR] MedSAM2 propagated mask not found at {pred_mask_path}\")\n",
    "        return\n",
    "\n",
    "    pred_mask = np.array(Image.open(pred_mask_path).convert('L'))\n",
    "    print(f\"[INFO] Loaded MedSAM2 mask from {pred_mask_path}\")\n",
    "\n",
    "    # Step 6: Compute Dice Score\n",
    "    dice = dice_score(pred_mask, gt_mask)\n",
    "    print(f\"[RESULT] Dice Score at frame {second_idx}: {dice:.4f}\")\n",
    "\n",
    "    # Step 7: Visualize\n",
    "    frame_img = np.array(Image.open(os.path.join(frames_dir, f\"{second_idx:03d}.jpg\")))\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].imshow(frame_img)\n",
    "    axs[0].imshow(gt_mask, alpha=0.4, cmap=\"Reds\")\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(frame_img)\n",
    "    axs[1].imshow(pred_mask, alpha=0.4, cmap=\"Greens\")\n",
    "    axs[1].set_title(\"SonoYolo + MedSAM2 Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Dice Score: {dice:.4f}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[DONE] Finished processing {video_filename}\")\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"{video_filename} - Frame {frame_idx}\")\n",
    "    plt.imshow(frame_img)\n",
    "    show_box(box, plt.gca())\n",
    "    show_mask((out_mask_logits[0] > 0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51eb84-b6ab-44e2-baa4-3578fb886a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Split Batch Evaluation\n",
    "\n",
    "from echonet.datasets import Echo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "\n",
    "# 1) Load EchoNet test split to get the .avi filenames\n",
    "dataset = Echo(\n",
    "    root=\"/EchoNet-Dynamic\",\n",
    "    split=\"test\",\n",
    "    target_type=\"Filename\",   # we only need filenames here\n",
    "    length=None,\n",
    "    clips=1\n",
    ")\n",
    "\n",
    "test_fnames = dataset.fnames   # list of 'XXXXXXXXXXXX.avi'\n",
    "\n",
    "print(f\"[INFO] Found {len(test_fnames)} test videos.\")\n",
    "\n",
    "# 2) Optional: suppress the per-video plt.show() to avoid popping windows / slowing down\n",
    "import contextlib, matplotlib.pyplot as plt\n",
    "@contextlib.contextmanager\n",
    "def suppress_plots():\n",
    "    _orig_show = plt.show\n",
    "    plt.show = lambda *a, **k: None\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        plt.show = _orig_show\n",
    "\n",
    "# 3) Run evaluation for each test video\n",
    "results = []\n",
    "start = time.time()\n",
    "\n",
    "with suppress_plots():   # remove this context manager if you DO want the plots\n",
    "    for i, fname in enumerate(test_fnames, 1):\n",
    "        try:\n",
    "            print(f\"\\n=== [{i}/{len(test_fnames)}] {fname} ===\")\n",
    "            dice = evaluate_medsam2_on_video(\n",
    "                video_filename=fname,\n",
    "                predictor=predictor,\n",
    "                # keep your defaults, or override here if needed:\n",
    "                tracings_csv=\"/EchoNet-Dynamic/VolumeTracings.csv\",\n",
    "                video_dir=\"/EchoNet-Dynamic/Videos\",\n",
    "                frames_root=\"/EchoNet-Dynamic/dataset_frames\",\n",
    "                masks_root=\"/EchoNet-Dynamic/yournameforthemasks\", #you must modify this\n",
    "                gt_mask_root=\"/EchoNet-Dynamic/manual_masks\", #your ground truth masks\n",
    "            )\n",
    "            results.append({\"filename\": fname, \"dice\": float(dice) if dice is not None else np.nan})\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {fname}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            results.append({\"filename\": fname, \"dice\": np.nan})\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n[SUMMARY] Finished {len(test_fnames)} videos in {elapsed/60:.1f} min.\")\n",
    "\n",
    "# 4) Save CSV + print mean Dice\n",
    "df_res = pd.DataFrame(results).sort_values(\"filename\")\n",
    "csv_path = \"/home/iraj/EchoNet-Dynamic/_____.csv\" #edit the csv path\n",
    "df_res.to_csv(csv_path, index=False)\n",
    "print(f\"[SUMMARY] Results saved to: {csv_path}\")\n",
    "print(f\"[SUMMARY] Mean Dice (ignoring NaN): {df_res['dice'].mean(skipna=True):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d6a59-9c71-4c6a-9a30-f82f22a67b3c",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18ac9e-92b8-42d7-8aab-4f91f7419151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dice_score(mask1, mask2):\n",
    "    mask1 = (mask1 > 0).astype(np.bool_)\n",
    "    mask2 = (mask2 > 0).astype(np.bool_)\n",
    "    intersection = np.sum(mask1 & mask2)\n",
    "    total = np.sum(mask1) + np.sum(mask2)\n",
    "    return 1.0 if total == 0 else 2.0 * intersection / total\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def evaluate_medsam2_on_video(video_filename, predictor,\n",
    "                               tracings_csv=\"/EchoNet-Dynamic/VolumeTracings.csv\",\n",
    "                               video_dir=\"/EchoNet-Dynamic/Videos\",\n",
    "                               frames_root=\"/EchoNet-Dynamic/dataset_frames_new\",\n",
    "                               masks_root=\"/EchoNet-Dynamic/yolomodmedsam2/backward_masks\",\n",
    "                               gt_mask_root=\"/EchoNet-Dynamic/manual_masks\",\n",
    "                               yolo_labels_dir=\"/runs/detect/predict/labels\",\n",
    "                               image_size=128):\n",
    "    \n",
    "    original_size = 112\n",
    "    padded_size = image_size\n",
    "    pad = (padded_size - original_size) // 2\n",
    "\n",
    "    print(f\"\\n========== Processing Video: {video_filename} ==========\")\n",
    "\n",
    "    video_id = os.path.splitext(video_filename)[0]\n",
    "    base_name = video_id\n",
    "    j_name = video_id + '.avi'\n",
    "    video_path = os.path.join(video_dir, video_filename)\n",
    "    frames_dir = os.path.join(frames_root, j_name)\n",
    "    masks_dir = os.path.join(masks_root, base_name)\n",
    "\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "    print(\"[INFO] Loading VolumeTracings.csv...\")\n",
    "    df = pd.read_csv(tracings_csv)\n",
    "    df_video = df[df['FileName'] == video_filename]\n",
    "    unique_frames = sorted(df_video['Frame'].unique())\n",
    "\n",
    "    if len(unique_frames) < 2:\n",
    "        print(f\"[WARNING] Not enough annotated frames in {base_name}\")\n",
    "        return\n",
    "\n",
    "    first_idx, second_idx = unique_frames[0], unique_frames[1]\n",
    "    print(f\"[INFO] First annotated frame: {first_idx}, Second annotated frame: {second_idx}\")\n",
    "\n",
    "    # =========== Unpad YOLO Prediction to Original Frame Size ============\n",
    "    label_filename = f\"{video_id}_frame{second_idx}.txt\"\n",
    "    label_path = os.path.join(yolo_labels_dir, label_filename)\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"[ERROR] YOLO label file not found: {label_path}\")\n",
    "        return\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        line = f.readline().strip()\n",
    "        parts = line.split()\n",
    "        if len(parts) != 5:\n",
    "            print(f\"[ERROR] Invalid YOLO label format in {label_path}\")\n",
    "            return\n",
    "        _, x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts)\n",
    "\n",
    "    # Convert to 128x128 pixel coordinates\n",
    "    x_center_128 = x_center_norm * padded_size\n",
    "    y_center_128 = y_center_norm * padded_size\n",
    "    width_128 = width_norm * padded_size\n",
    "    height_128 = height_norm * padded_size\n",
    "\n",
    "    # Shift center back by -pad to align with 112x112 original frame\n",
    "    x_center_112 = x_center_128 - pad\n",
    "    y_center_112 = y_center_128 - pad\n",
    "\n",
    "    # Final box in original 112x112 space\n",
    "    xmin = x_center_112 - width_128 / 2\n",
    "    ymin = y_center_112 - height_128 / 2\n",
    "    xmax = x_center_112 + width_128 / 2\n",
    "    ymax = y_center_112 + height_128 / 2\n",
    "    box = np.array([xmin+3, ymin+3, xmax-3, ymax-3], dtype=np.float32)\n",
    "\n",
    "    print(f\"[INFO] Unpadded YOLO bbox for MedSAM2: {box}\")\n",
    "    # =====================================================================\n",
    "\n",
    "    # Step 1: Extract frames\n",
    "    if len(os.listdir(frames_dir)) == 0:\n",
    "        print(f\"[INFO] Extracting frames from {video_path}...\")\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_path = os.path.join(frames_dir, f\"{idx}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            idx += 1\n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {idx} frames.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Frames already present in {frames_dir}\")\n",
    "\n",
    "    # Step 2: Initialize and run MedSAM2\n",
    "    print(f\"[INFO] Initializing MedSAM2 predictor...\")\n",
    "    inference_state = predictor.init_state(video_path=frames_dir)\n",
    "    predictor.reset_state(inference_state)\n",
    "\n",
    "    print(f\"[INFO] Running MedSAM2 at frame {second_idx}...\")\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=second_idx,\n",
    "        obj_id=1,\n",
    "        box=box,\n",
    "    )\n",
    "\n",
    "    # Step 3: Propagate masks\n",
    "    print(\"[INFO] Propagating masks backward...\")\n",
    "    steps = (second_idx - first_idx + 1) if second_idx >= first_idx else None  # None = let it run to start\n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n",
    "            inference_state,\n",
    "            start_frame_idx=second_idx,                 # start where we seeded\n",
    "            max_frame_num_to_track=steps,               # just enough to reach first_idx\n",
    "            reverse=True):                              #go backward\n",
    "        for i, out_obj_id in enumerate(out_obj_ids):\n",
    "            mask = (out_mask_logits[i] > 0).cpu().numpy().squeeze().astype(np.uint8) * 255\n",
    "            mask_path = os.path.join(masks_dir, f\"frame_{out_frame_idx}_sam_mask.png\")\n",
    "            Image.fromarray(mask).save(mask_path)\n",
    "            if out_frame_idx % 30 == 0:\n",
    "                print(f\"[DEBUG] Saved MedSAM2 mask at frame {out_frame_idx}\")\n",
    "\n",
    "    # Step 4: Generate ground truth mask(s) for this video\n",
    "    print(f\"[INFO] Generating ground truth mask(s) for {video_filename}...\")\n",
    "    save_manual_masks(video_filename)\n",
    "        \n",
    "    \n",
    "    # load ground truth mask for the second annotated frame ----\n",
    "    gt_mask_path = os.path.join(gt_mask_root, video_filename, f\"frame_{first_idx:03d}_manual_mask.png\")\n",
    "    if not os.path.exists(gt_mask_path):\n",
    "        print(f\"[ERROR] Ground truth mask not found at {gt_mask_path}\")\n",
    "        return\n",
    "    gt_mask = np.array(Image.open(gt_mask_path).convert('L'))\n",
    "    print(f\"[INFO] Loaded ground truth mask from {gt_mask_path}\")\n",
    "\n",
    "    # Step 5: Load MedSAM2 mask\n",
    "    pred_mask_path = os.path.join(masks_dir, f\"frame_{first_idx}_sam_mask.png\")\n",
    "    if not os.path.exists(pred_mask_path):\n",
    "        print(f\"[ERROR] MedSAM2 propagated mask not found at {pred_mask_path}\")\n",
    "        return\n",
    "\n",
    "    pred_mask = np.array(Image.open(pred_mask_path).convert('L'))\n",
    "    print(f\"[INFO] Loaded MedSAM2 mask from {pred_mask_path}\")\n",
    "\n",
    "    # Step 6: Compute Dice Score\n",
    "    dice = dice_score(pred_mask, gt_mask)\n",
    "    print(f\"[RESULT] Dice Score at frame {first_idx}: {dice:.4f}\")\n",
    "\n",
    "    # Step 7: Visualize\n",
    "    frame_img = np.array(Image.open(os.path.join(frames_dir, f\"{first_idx:03d}.jpg\")))\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].imshow(frame_img)\n",
    "    axs[0].imshow(gt_mask, alpha=0.4, cmap=\"Reds\")\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(frame_img)\n",
    "    axs[1].imshow(pred_mask, alpha=0.4, cmap=\"Greens\")\n",
    "    axs[1].set_title(\"SonoYolo + MedSAM2 Prediction\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"{video_filename}  |  Backward Dice @ frame {first_idx}: {dice:.4f}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"[DONE] Finished processing {video_filename}\")\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.title(f\"{video_filename} - Frame {frame_idx}\")\n",
    "    plt.imshow(frame_img)\n",
    "    show_box(box, plt.gca())\n",
    "    show_mask((out_mask_logits[0] > 0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce515d-6b76-439c-9996-b34ecba71e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test Split Batch Evaluation\n",
    "from echonet.datasets import Echo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# 1) Load EchoNet test split to get the .avi filenames\n",
    "dataset = Echo(\n",
    "    root=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic\",\n",
    "    split=\"test\",\n",
    "    target_type=\"Filename\",   # we only need filenames here\n",
    "    length=None,\n",
    "    clips=1\n",
    ")\n",
    "\n",
    "test_fnames = dataset.fnames   # list of 'XXXXXXXXXXXX.avi'\n",
    "\n",
    "print(f\"[INFO] Found {len(test_fnames)} test videos.\")\n",
    "\n",
    "# 2) Optional: suppress the per-video plt.show() to avoid popping windows / slowing down\n",
    "import contextlib, matplotlib.pyplot as plt\n",
    "@contextlib.contextmanager\n",
    "def suppress_plots():\n",
    "    _orig_show = plt.show\n",
    "    plt.show = lambda *a, **k: None\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        plt.show = _orig_show\n",
    "\n",
    "# 3) Run evaluation for each test video\n",
    "results = []\n",
    "start = time.time()\n",
    "\n",
    "with suppress_plots():   \n",
    "    for i, fname in enumerate(test_fnames, 1):\n",
    "        try:\n",
    "            print(f\"\\n=== [{i}/{len(test_fnames)}] {fname} ===\")\n",
    "            dice = evaluate_medsam2_on_video(\n",
    "                video_filename=fname,\n",
    "                predictor=predictor,\n",
    "           \n",
    "                tracings_csv=\"/EchoNet-Dynamic/VolumeTracings.csv\",\n",
    "                video_dir=\"/EchoNet-Dynamic/Videos\",\n",
    "                frames_root=\"/EchoNet-Dynamic/dataset_frames_new\",\n",
    "                masks_root=\"/EchoNet-Dynamic/yolomodmedsam2/backward_masks\",\n",
    "                gt_mask_root=\"/EchoNet-Dynamic/manual_masks\",\n",
    "            )\n",
    "            results.append({\"filename\": fname, \"dice\": float(dice) if dice is not None else np.nan})\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {fname}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            results.append({\"filename\": fname, \"dice\": np.nan})\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n[SUMMARY] Finished {len(test_fnames)} videos in {elapsed/60:.1f} min.\")\n",
    "\n",
    "# 4) Save CSV + print mean Dice\n",
    "df_res = pd.DataFrame(results).sort_values(\"filename\")\n",
    "csv_path = \"/EchoNet-Dynamic/yolomodmedsam2_backward_masks1.csv\"\n",
    "df_res.to_csv(csv_path, index=False)\n",
    "print(f\"[SUMMARY] Results saved to: {csv_path}\")\n",
    "print(f\"[SUMMARY] Mean Dice (ignoring NaN): {df_res['dice'].mean(skipna=True):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621622b7-2b5e-4f39-b646-7a1343f9a87b",
   "metadata": {},
   "source": [
    "# Framewise segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fc97a-2ce8-426e-814c-fcc6825d1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dice_score(mask1, mask2):\n",
    "    mask1 = (mask1 > 0).astype(np.bool_)\n",
    "    mask2 = (mask2 > 0).astype(np.bool_)\n",
    "    intersection = np.sum(mask1 & mask2)\n",
    "    total = np.sum(mask1) + np.sum(mask2)\n",
    "    return 1.0 if total == 0 else 2.0 * intersection / total\n",
    "\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def evaluate_medsam2_on_video(video_filename, predictor,\n",
    "                              tracings_csv=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/VolumeTracings.csv\",\n",
    "                              video_dir=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/Videos\",\n",
    "                              frames_root=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/dataset_frames_new\",\n",
    "                              masks_root=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/yolomodmedsam2/framewise\",\n",
    "                              gt_mask_root=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/manual_masks\",\n",
    "                              yolo_labels_dir=\"/home/iraj/runs/detect/predict4/labels\",\n",
    "                              image_size=128):\n",
    "    \"\"\"\n",
    "    Same structure as your original function, but:\n",
    "      - Uses YOLO bbox *per annotated frame* (no propagation).\n",
    "      - Converts YOLO (128x128 padded) coords back to 112x112 frame coords.\n",
    "      - Saves per-frame masks and computes Dice vs GT for the two annotated frames.\n",
    "      - Returns a single numeric average Dice.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n========== Processing Video (frame-wise): {video_filename} ==========\")\n",
    "\n",
    "    base_name  = os.path.splitext(video_filename)[0]\n",
    "    video_path = os.path.join(video_dir, video_filename)\n",
    "    frames_dir = os.path.join(frames_root, video_filename)   # keep your layout\n",
    "    masks_dir  = os.path.join(masks_root, base_name)\n",
    "\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    os.makedirs(masks_dir,  exist_ok=True)\n",
    "\n",
    "    # --- helpers (kept inside, like your original bbox helper) ---\n",
    "    def _yolo_label_path(video_id, frame_idx):\n",
    "        # Expect: {video_id}_frame{frame_idx}.txt  (e.g., 0XABC..._frame154.txt)\n",
    "        return os.path.join(yolo_labels_dir, f\"{video_id}_frame{frame_idx}.txt\")\n",
    "\n",
    "    def _yolo_box_to_112_coords(label_path, padded_size=128, original_size=112):\n",
    "        \"\"\"Read first bbox from YOLO txt and convert from padded_size->original_size coords.\"\"\"\n",
    "        pad = (padded_size - original_size) // 2\n",
    "        with open(label_path, \"r\") as f:\n",
    "            line = f.readline().strip()\n",
    "            parts = line.split()\n",
    "            if len(parts) < 5:\n",
    "                raise ValueError(f\"Invalid YOLO label format in {label_path}\")\n",
    "            # YOLO format: cls cx cy w h (normalized)\n",
    "            _, x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts[:5])\n",
    "\n",
    "        x_center_128 = x_center_norm * padded_size\n",
    "        y_center_128 = y_center_norm * padded_size\n",
    "        width_128    = width_norm    * padded_size\n",
    "        height_128   = height_norm   * padded_size\n",
    "\n",
    "        # shift center back by -pad to align with 112x112 content\n",
    "        x_center_112 = x_center_128 - pad\n",
    "        y_center_112 = y_center_128 - pad\n",
    "\n",
    "        xmin = x_center_112 - width_128  / 2.0\n",
    "        ymin = y_center_112 - height_128 / 2.0\n",
    "        xmax = x_center_112 + width_128  / 2.0\n",
    "        ymax = y_center_112 + height_128 / 2.0\n",
    "\n",
    "        # clip bounds\n",
    "        xmin = max(0, min(xmin, original_size - 1))\n",
    "        ymin = max(0, min(ymin, original_size - 1))\n",
    "        xmax = max(0, min(xmax, original_size - 1))\n",
    "        ymax = max(0, min(ymax, original_size - 1))\n",
    "\n",
    "        return np.array([xmin+3, ymin+3, xmax-3, ymax-3], dtype=np.float32)\n",
    "\n",
    "    # 1) Load annotated frames (two) from VolumeTracings.csv\n",
    "    print(\"[INFO] Loading VolumeTracings.csv...\")\n",
    "    df = pd.read_csv(tracings_csv)\n",
    "    df_video = df[df['FileName'] == video_filename]\n",
    "    unique_frames = sorted(df_video['Frame'].unique())\n",
    "\n",
    "    if len(unique_frames) < 2:\n",
    "        print(f\"[WARNING] Not enough annotated frames in {base_name}\")\n",
    "        return np.nan  # keep return type numeric\n",
    "\n",
    "    first_idx, second_idx = int(unique_frames[0]), int(unique_frames[1])\n",
    "    print(f\"[INFO] Annotated frames: [{first_idx}, {second_idx}]\")\n",
    "\n",
    "    # 2) Extract frames if needed\n",
    "    if len([f for f in os.listdir(frames_dir) if f.endswith(\".jpg\")]) == 0:\n",
    "        print(f\"[INFO] Extracting frames from {video_path}...\")\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            cv2.imwrite(os.path.join(frames_dir, f\"{idx:03d}.jpg\"), frame)\n",
    "            idx += 1\n",
    "        cap.release()\n",
    "        print(f\"[INFO] Extracted {idx} frames.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Frames already present in {frames_dir}\")\n",
    "\n",
    "    # 3) Ensure GT masks exist (uses your helper if present)\n",
    "    print(f\"[INFO] Ensuring ground-truth masks exist for {video_filename}...\")\n",
    "    try:\n",
    "        save_manual_masks(video_filename)  # your existing helper\n",
    "    except NameError:\n",
    "        print(\"[WARN] save_manual_masks() not found; assuming GT masks already exist.\")\n",
    "\n",
    "    # Single-frame inference (NO propagation/history), using YOLO bbox for THIS frame\n",
    "    def run_single_frame_with_yolo(frame_idx, obj_id=1):\n",
    "        label_path = _yolo_label_path(base_name, frame_idx)\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"[ERROR] YOLO label not found for frame {frame_idx}: {label_path}\")\n",
    "            return None, np.nan\n",
    "\n",
    "        try:\n",
    "            box = _yolo_box_to_112_coords(label_path, padded_size=image_size, original_size=112)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to parse YOLO bbox for frame {frame_idx}: {e}\")\n",
    "            return None, np.nan\n",
    "\n",
    "        print(f\"[INFO] (Frame {frame_idx}) YOLO BBox for MedSAM2 (112x112): {box}\")\n",
    "\n",
    "        # Initialize/reset predictor state so nothing propagates\n",
    "        inference_state = predictor.init_state(video_path=frames_dir)\n",
    "        predictor.reset_state(inference_state)\n",
    "\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=int(frame_idx),\n",
    "            obj_id=obj_id,\n",
    "            box=box,\n",
    "        )\n",
    "\n",
    "        # pick logits for obj_id if multiple\n",
    "        if hasattr(out_mask_logits, \"__len__\") and len(out_mask_logits) > 1:\n",
    "            try:\n",
    "                idx = list(out_obj_ids).index(obj_id)\n",
    "                logits = out_mask_logits[idx]\n",
    "            except Exception:\n",
    "                logits = out_mask_logits[0]\n",
    "        else:\n",
    "            logits = out_mask_logits[0] if hasattr(out_mask_logits, \"__getitem__\") else out_mask_logits\n",
    "\n",
    "        mask = (logits > 0).detach().cpu().numpy().squeeze().astype(np.uint8) * 255\n",
    "        pred_mask_path = os.path.join(masks_dir, f\"frame_{frame_idx}_sam_mask.png\")\n",
    "        Image.fromarray(mask).save(pred_mask_path)\n",
    "        print(f\"[SAVED] MedSAM2 mask → {pred_mask_path}\")\n",
    "        return pred_mask_path, mask\n",
    "\n",
    "    # 4) Run on both annotated frames (independently)\n",
    "    pred1_path, pm1 = run_single_frame_with_yolo(first_idx,  obj_id=1)\n",
    "    pred2_path, pm2 = run_single_frame_with_yolo(second_idx, obj_id=1)\n",
    "\n",
    "    # 5) Load GT and compute Dice per frame\n",
    "    gt1_path = os.path.join(gt_mask_root, video_filename, f\"frame_{first_idx:03d}_manual_mask.png\")\n",
    "    gt2_path = os.path.join(gt_mask_root, video_filename, f\"frame_{second_idx:03d}_manual_mask.png\")\n",
    "\n",
    "    if not (os.path.exists(gt1_path) and os.path.exists(gt2_path)):\n",
    "        print(f\"[ERROR] Missing GT mask(s). Expected:\\n  {gt1_path}\\n  {gt2_path}\")\n",
    "        return np.nan\n",
    "\n",
    "    gt1 = np.array(Image.open(gt1_path).convert('L'))\n",
    "    gt2 = np.array(Image.open(gt2_path).convert('L'))\n",
    "\n",
    "    # Handle any failure that returned NaN\n",
    "    if pm1 is None or pm2 is None:\n",
    "        dice_vals = []\n",
    "        if pm1 is not None:\n",
    "            dice_vals.append(dice_score(pm1, gt1))\n",
    "        if pm2 is not None:\n",
    "            dice_vals.append(dice_score(pm2, gt2))\n",
    "        avg_dice = float(np.mean(dice_vals)) if len(dice_vals) else np.nan\n",
    "        print(f\"[RESULT] Partial/NaN dice due to missing predictions. Avg: {avg_dice}\")\n",
    "        return avg_dice\n",
    "\n",
    "    dice1 = dice_score(pm1, gt1)\n",
    "    dice2 = dice_score(pm2, gt2)\n",
    "\n",
    "    print(f\"[RESULT] Dice (frame {first_idx:03d}):  {dice1:.4f}\")\n",
    "    print(f\"[RESULT] Dice (frame {second_idx:03d}): {dice2:.4f}\")\n",
    "\n",
    "    avg_dice = float((dice1 + dice2) / 2.0)\n",
    "    print(f\"[RESULT] Average Dice (two annotated frames): {avg_dice:.4f}\")\n",
    "\n",
    "    # 6) Visualization blocks (same structure as your original)\n",
    "    try:\n",
    "        frame_img1 = np.array(Image.open(os.path.join(frames_dir, f\"{first_idx:03d}.jpg\")))\n",
    "        fig1, axs1 = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        axs1[0].imshow(frame_img1); axs1[0].imshow(gt1, alpha=0.5, cmap=\"Reds\")\n",
    "        axs1[0].set_title(f\"GT (frame {first_idx:03d})\"); axs1[0].axis(\"off\")\n",
    "        axs1[1].imshow(frame_img1); axs1[1].imshow(pm1, alpha=0.5, cmap=\"Greens\")\n",
    "        axs1[1].set_title(\"MedSAM2 Prediction\"); axs1[1].axis(\"off\")\n",
    "        plt.suptitle(f\"{video_filename} | Dice (frame {first_idx:03d}) = {dice1:.4f}\", fontsize=14)\n",
    "        plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Visualization skipped for frame {first_idx}: {e}\")\n",
    "\n",
    "    try:\n",
    "        frame_img2 = np.array(Image.open(os.path.join(frames_dir, f\"{second_idx:03d}.jpg\")))\n",
    "        fig2, axs2 = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        axs2[0].imshow(frame_img2); axs2[0].imshow(gt2, alpha=0.5, cmap=\"Reds\")\n",
    "        axs2[0].set_title(f\"GT (frame {second_idx:03d})\"); axs2[0].axis(\"off\")\n",
    "        axs2[1].imshow(frame_img2); axs2[1].imshow(pm2, alpha=0.5, cmap=\"Greens\")\n",
    "        axs2[1].set_title(\"MedSAM2 Prediction\"); axs2[1].axis(\"off\")\n",
    "        plt.suptitle(f\"{video_filename} | Dice {first_idx:03d}: {dice1:.4f} | {second_idx:03d}: {dice2:.4f} | Avg: {avg_dice:.4f}\", fontsize=14)\n",
    "        plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Visualization skipped: {e}\")\n",
    "\n",
    "    print(f\"[DONE] Finished (frame-wise) {video_filename}\")\n",
    "    return avg_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7b56b-4c18-478b-92ee-1b895f797c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test Split Batch Evaluation\n",
    "\n",
    "from echonet.datasets import Echo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "\n",
    "# 1) Load EchoNet test split to get the .avi filenames\n",
    "dataset = Echo(\n",
    "    root=\"/shared_data/p_vidalr/iraj/EchoNet-Dynamic\",\n",
    "    split=\"test\",\n",
    "    target_type=\"Filename\",   # we only need filenames here\n",
    "    length=None,\n",
    "    clips=1\n",
    ")\n",
    "\n",
    "test_fnames = dataset.fnames   # list of 'XXXXXXXXXXXX.avi'\n",
    "\n",
    "print(f\"[INFO] Found {len(test_fnames)} test videos.\")\n",
    "\n",
    "# 2) Optional: suppress the per-video plt.show() to avoid popping windows / slowing down\n",
    "import contextlib, matplotlib.pyplot as plt\n",
    "@contextlib.contextmanager\n",
    "def suppress_plots():\n",
    "    _orig_show = plt.show\n",
    "    plt.show = lambda *a, **k: None\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        plt.show = _orig_show\n",
    "\n",
    "# 3) Run evaluation for each test video\n",
    "results = []\n",
    "start = time.time()\n",
    "\n",
    "with suppress_plots():   # remove this context manager if you DO want the plots\n",
    "    for i, fname in enumerate(test_fnames, 1):\n",
    "        try:\n",
    "            print(f\"\\n=== [{i}/{len(test_fnames)}] {fname} ===\")\n",
    "            dice = evaluate_medsam2_on_video(\n",
    "                video_filename=fname,\n",
    "                predictor=predictor,\n",
    "                # keep your defaults, or override here if needed:\n",
    "                tracings_csv=\"/EchoNet-Dynamic/VolumeTracings.csv\",\n",
    "                video_dir=\"/EchoNet-Dynamic/Videos\",\n",
    "                frames_root=\"/EchoNet-Dynamic/dataset_frames_new\",\n",
    "                masks_root=\"/EchoNet-Dynamic/yolomodmedsam2/framewise\",\n",
    "                gt_mask_root=\"/EchoNet-Dynamic/manual_masks\",\n",
    "            )\n",
    "            results.append({\"filename\": fname, \"dice\": float(dice) if dice is not None else np.nan})\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {fname}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            results.append({\"filename\": fname, \"dice\": np.nan})\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n[SUMMARY] Finished {len(test_fnames)} videos in {elapsed/60:.1f} min.\")\n",
    "\n",
    "# 4) Save CSV + print mean Dice\n",
    "df_res = pd.DataFrame(results).sort_values(\"filename\")\n",
    "csv_path = \"/EchoNet-Dynamic/yolomodmedsam2_framewise.csv\"\n",
    "df_res.to_csv(csv_path, index=False)\n",
    "print(f\"[SUMMARY] Results saved to: {csv_path}\")\n",
    "print(f\"[SUMMARY] Mean Dice (ignoring NaN): {df_res['dice'].mean(skipna=True):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam2",
   "language": "python",
   "name": "medsam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
