{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712420fa-da4e-4c16-8d3f-49fe4d74754d",
   "metadata": {},
   "source": [
    "# SonoYOLO Finetuning YOLOv11n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef0792-a725-425d-ba92-f8a9904beb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948773d6-31f8-4ecf-929d-af50dc564030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613bfa5-8f29-4603-8525-4849fdc84239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell prepares the data for training\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "video_dir = \"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/Videos\"\n",
    "output_root = \"echonet_yolo\"\n",
    "original_size = 112         # Original size of frames (EchoNet-Dynamic Dataset\n",
    "padded_size = 128           # Padded file size for YOLOv11n\n",
    "pad = (padded_size - original_size) // 2  # 8 pixels on each side\n",
    "\n",
    "vol_csv = \"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/VolumeTracings.csv\" #Volume Tracings File in Stanford Echonet-Dynamic Dataset File\n",
    "filelist_csv = \"/shared_data/p_vidalr/iraj/EchoNet-Dynamic/FileList.csv\" #We are using the splits that the Stanford EchoNet-Dynamic Model used for training, testing and validation\n",
    "\n",
    "# --- Create output folders ---\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(output_root, f\"images/{split}\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_root, f\"labels/{split}\"), exist_ok=True)\n",
    "\n",
    "# --- Load annotations (VolumeTracings) ---\n",
    "df = pd.read_csv(vol_csv)\n",
    "if \"FileName\" in df.columns:\n",
    "    tracings_file_col = \"FileName\"\n",
    "elif \"Filename\" in df.columns:\n",
    "    tracings_file_col = \"Filename\"\n",
    "else:\n",
    "    raise ValueError(\"VolumeTracings.csv must contain 'FileName' or 'Filename' column.\")\n",
    "\n",
    "df[tracings_file_col] = df[tracings_file_col].astype(str).str.strip()\n",
    "\n",
    "# --- Load EchoNet-style split list (FileList.csv) ---\n",
    "fl = pd.read_csv(filelist_csv)\n",
    "\n",
    "\n",
    "if \"FileName\" in fl.columns:\n",
    "    filelist_file_col = \"FileName\"\n",
    "elif \"Filename\" in fl.columns:\n",
    "    filelist_file_col = \"Filename\"\n",
    "else:\n",
    "    \n",
    "    filelist_file_col = fl.columns[0]\n",
    "\n",
    "if \"Split\" in fl.columns:\n",
    "    split_col = \"Split\"\n",
    "elif \"split\" in fl.columns:\n",
    "    split_col = \"split\"\n",
    "else:\n",
    "    raise ValueError(\"FileList.csv must contain 'Split' (or 'split') column with values TRAIN/VAL/TEST.\")\n",
    "\n",
    "# Normalize filenames (as given, these are basenames without extension)\n",
    "fl[filelist_file_col] = fl[filelist_file_col].astype(str).str.strip()\n",
    "\n",
    "# Normalize split labels from TRAIN/VAL/TEST -> train/val/test\n",
    "fl[split_col] = (\n",
    "    fl[split_col]\n",
    "    .astype(str).str.strip().str.upper()\n",
    "    .map({\"TRAIN\": \"train\", \"VAL\": \"val\", \"TEST\": \"test\"})\n",
    ")\n",
    "\n",
    "\n",
    "fl[\"_with_ext\"] = fl[filelist_file_col] + \".avi\"\n",
    "\n",
    "# dataset splits from FileList.csv\n",
    "splits = {\n",
    "    'train': sorted(fl.loc[fl[split_col] == 'train', \"_with_ext\"].unique().tolist()),\n",
    "    'val':   sorted(fl.loc[fl[split_col] == 'val',   \"_with_ext\"].unique().tolist()),\n",
    "    'test':  sorted(fl.loc[fl[split_col] == 'test',  \"_with_ext\"].unique().tolist()),\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in splits.items()})\n",
    "\n",
    "# --- Pre-index tracings per video for speed ---\n",
    "grouped = {k: g for k, g in df.groupby(tracings_file_col)}\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "for split, video_list in splits.items():\n",
    "    img_dir = os.path.join(output_root, f\"images/{split}\")\n",
    "    lbl_dir = os.path.join(output_root, f\"labels/{split}\")\n",
    "\n",
    "    for video_name in tqdm(video_list, desc=f\"Processing {split}\"):\n",
    "        # Skip if no tracings (paranoia; we already filtered)\n",
    "        if video_name not in grouped:\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, video_name)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open {video_name}\")\n",
    "            continue\n",
    "\n",
    "        # Get annotated frames for this video\n",
    "        frames_df = grouped[video_name]\n",
    "        unique_frames = frames_df['Frame'].dropna().astype(int).unique()\n",
    "        if len(unique_frames) < 2:\n",
    "            cap.release()\n",
    "            continue\n",
    "\n",
    "        # Choose first 2 annotated frames (ED/ES typically present)\n",
    "        selected_frames = sorted(unique_frames)[:2]\n",
    "\n",
    "        for frame_num in selected_frames:\n",
    "            frame_rows = frames_df[frames_df['Frame'].astype(int) == frame_num]\n",
    "            if frame_rows.empty:\n",
    "                continue\n",
    "\n",
    "            # Get bbox corners from tracings\n",
    "            x_vals = pd.concat([frame_rows['X1'], frame_rows['X2']])\n",
    "            y_vals = pd.concat([frame_rows['Y1'], frame_rows['Y2']])\n",
    "\n",
    "            xmin, xmax = int(x_vals.min()), int(x_vals.max())\n",
    "            ymin, ymax = int(y_vals.min()), int(y_vals.max())\n",
    "\n",
    "            # Read frame (EchoNet videos are already 112x112)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_num))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Could not read frame {frame_num} in {video_name}\")\n",
    "                continue\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # === Pad to 128Ã—128 ===\n",
    "            frame_padded = cv2.copyMakeBorder(\n",
    "                frame_rgb, pad, pad, pad, pad,\n",
    "                borderType=cv2.BORDER_CONSTANT,\n",
    "                value=(0, 0, 0)\n",
    "            )\n",
    "\n",
    "            # === Adjust bounding box (shift by pad; normalize by 128) ===\n",
    "            x_center = ((xmin + xmax) / 2 + pad) / padded_size\n",
    "            y_center = ((ymin + ymax) / 2 + pad) / padded_size\n",
    "            width = (xmax - xmin) / padded_size\n",
    "            height = (ymax - ymin) / padded_size\n",
    "\n",
    "            # === Save image and label ===\n",
    "            base_name = f\"{video_name.replace('.avi', '')}_frame{frame_num}\"\n",
    "            image_path = os.path.join(img_dir, f\"{base_name}.jpg\")\n",
    "            label_path = os.path.join(lbl_dir, f\"{base_name}.txt\")\n",
    "\n",
    "            cv2.imwrite(image_path, frame_padded)\n",
    "            with open(label_path, \"w\") as f:\n",
    "                f.write(f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8761a-d4eb-47b9-875a-77e44c0e8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Edit based on your needs\n",
    "DATA_YAML = \"/home/iraj/echonet_yolo/echonet_yolo.yaml\"\n",
    "TRAIN_EPOCHS = 40\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_WEIGHTS = \"yolo11n.pt\"  # pretrained weights to start from\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "print(\"Loading pretrained YOLOv11n model\")\n",
    "model = YOLO(TRAIN_WEIGHTS)\n",
    "\n",
    "print(f\"Training model on dataset: {DATA_YAML}\")\n",
    "model.train(\n",
    "    data=DATA_YAML,\n",
    "    epochs=TRAIN_EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    pretrained=True  # use pretrained weights and adapt head\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9f451-6417-4ee2-bcac-94257b5bb65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_weights_path = \"/home/iraj/runs/detect/train4/weights/best.pt\" #after training you should get best weights\n",
    "model = YOLO(best_weights_path)\n",
    "\n",
    "# Run inference on a single image or folder of images\n",
    "results = model.predict(\n",
    "    source=\"/home/iraj/echonet_yolo/images/test\",  # can be image, folder, or video\n",
    "    imgsz= 128,  # keep consistent with training image size\n",
    "    conf=0.65,  # confidence threshold\n",
    "    save=True,  # save the results to 'runs/detect/exp' folder by default\n",
    "    save_txt=True  # optionally save prediction labels in txt files\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo11s-env",
   "language": "python",
   "name": "yolo11s-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
